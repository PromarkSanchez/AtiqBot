# app/api/endpoints/chat_api_endpoints.py
import os
import time
import traceback
from typing import List, Optional, Dict, Any, Union, Type
import asyncio
import json

from fastapi import APIRouter, HTTPException, Depends, status
from sqlalchemy import select, create_engine, Column, Integer, Text, DateTime, String, JSON as SQLA_JSON_TYPE
from sqlalchemy.orm import Session, declarative_base, sessionmaker
from sqlalchemy.sql import func
from sqlalchemy.ext.asyncio import AsyncSession
from dotenv import load_dotenv
import google.generativeai as genai

# Langchain Imports
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, messages_from_dict, message_to_dict
from langchain_core.documents import Document as LangchainDocument
from langchain_postgres.vectorstores import PGVector
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.llm import LLMChain
from langchain.chains.question_answering import load_qa_chain

# Application Specific Imports
from app.db.session import get_crud_db_session
from app.models.api_client import ApiClient as ApiClientModel
from app.models.context_definition import ContextDefinition, ContextMainType
from app.models.db_connection_config import DatabaseConnectionConfig
from app.schemas.schemas import ChatRequest, ChatResponse
from app.crud import crud_interaction_log, crud_context_definition
from app.crud.crud_db_connection import get_db_connection_by_id_sync
from app.config import settings
from app.tools.sql_tools import run_text_to_sql_lcel_chain
from app.security.api_key_auth import get_validated_api_client

print("CHAT_EP_DEBUG: chat_api_endpoints.py module loading...")

# --- Constantes del Módulo ---
MODEL_NAME_SBERT_FOR_EMBEDDING = settings.MODEL_NAME_SBERT_FOR_EMBEDDING
PGVECTOR_CHAT_COLLECTION_NAME = settings.PGVECTOR_CHAT_COLLECTION_NAME
ACTIVE_LLM_MODEL_NAME = settings.DEFAULT_LLM_MODEL_NAME
MAX_RETRIEVED_CHUNKS_RAG = settings.MAX_RETRIEVED_CHUNKS_RAG
CHAT_HISTORY_TABLE_NAME = settings.CHAT_HISTORY_TABLE_NAME
DW_CONTEXT_CONFIG_NAME = settings.DW_CONTEXT_CONFIG_NAME
CHAT_HISTORY_WINDOW_SIZE_RAG = settings.CHAT_HISTORY_WINDOW_SIZE_RAG
CHAT_HISTORY_WINDOW_SIZE_SQL = settings.CHAT_HISTORY_WINDOW_SIZE_SQL
LANGCHAIN_VERBOSE_FLAG = settings.LANGCHAIN_VERBOSE
SQL_INTENT_KEYWORDS = settings.SQL_INTENT_KEYWORDS
DW_TABLE_PREFIXES_FOR_INTENT = settings.DW_TABLE_PREFIXES_FOR_INTENT

# --- Configuración de Gemini ---
load_dotenv()
GEMINI_API_KEY_ENV = settings.GEMINI_API_KEY
if not GEMINI_API_KEY_ENV: print("CHAT_EP_WARNING: GEMINI_API_KEY no encontrado.")
else:
    try: genai.configure(api_key=GEMINI_API_KEY_ENV); print("CHAT_EP_INFO: Google Generative AI (Gemini) configurado.")
    except Exception as e: print(f"CHAT_EP_ERROR: Configurando genai: {e}")

router = APIRouter(prefix="/api/v1/chat", tags=["Chat"])

# --- Instancias Singleton ---
_llm_chat_instance: Optional[ChatGoogleGenerativeAI] = None
_vector_store_instance_sync: Optional[PGVector] = None
_lc_sbert_embeddings_instance: Optional[SentenceTransformerEmbeddings] = None
_sync_history_engine = None 
_SyncHistorySessionLocalFactory = None 

# --- Funciones de obtención para Singletons (sin cambios) ---
def get_lc_sbert_embeddings() -> SentenceTransformerEmbeddings:
    global _lc_sbert_embeddings_instance
    if _lc_sbert_embeddings_instance is None:
        print(f"CHAT_EP_INFO: Creando SBERT Embeddings instance: {MODEL_NAME_SBERT_FOR_EMBEDDING}")
        _lc_sbert_embeddings_instance = SentenceTransformerEmbeddings(model_name=MODEL_NAME_SBERT_FOR_EMBEDDING)
    return _lc_sbert_embeddings_instance

def get_langchain_llm() -> ChatGoogleGenerativeAI:
    global _llm_chat_instance
    if _llm_chat_instance is None:
        print(f"CHAT_EP_INFO: Creando ChatGoogleGenerativeAI LLM instance: {ACTIVE_LLM_MODEL_NAME}")
        if not GEMINI_API_KEY_ENV: raise ValueError("GEMINI_API_KEY es requerido para instanciar el LLM.")
        _llm_chat_instance = ChatGoogleGenerativeAI(model=ACTIVE_LLM_MODEL_NAME, google_api_key=GEMINI_API_KEY_ENV, temperature=settings.DEFAULT_LLM_TEMPERATURE)
        print("CHAT_EP_INFO: ChatGoogleGenerativeAI LLM instance creada.")
    return _llm_chat_instance

def get_chat_vector_store_sync() -> PGVector:
    global _vector_store_instance_sync
    if _vector_store_instance_sync is None:
        print(f"CHAT_EP_INFO: Creando PGVector (síncrono) instance. Collection: '{PGVECTOR_CHAT_COLLECTION_NAME}'")
        lc_embeddings = get_lc_sbert_embeddings()
        sync_vector_db_url = settings.SYNC_DATABASE_VECTOR_URL
        if not sync_vector_db_url: raise ValueError("SYNC_DATABASE_VECTOR_URL no configurada para PGVector.")
        _vector_store_instance_sync = PGVector(connection=sync_vector_db_url, embeddings=lc_embeddings, collection_name=PGVECTOR_CHAT_COLLECTION_NAME, use_jsonb=True, async_mode=False, create_extension=False)
        print(f"CHAT_EP_INFO: PGVector (síncrono) instance creada para '{PGVECTOR_CHAT_COLLECTION_NAME}'.")
    return _vector_store_instance_sync

# --- Implementación Personalizada del Historial de Chat (FullyCustomChatMessageHistory) ---
_HistoryBase = declarative_base()
class _HistoryMessageORM(_HistoryBase): # Tu modelo ORM actual para el historial
    __tablename__ = CHAT_HISTORY_TABLE_NAME
    id = Column(Integer, primary_key=True, index=True, autoincrement=True)
    session_id = Column(String(255), nullable=False, index=True)
    message = Column(SQLA_JSON_TYPE, nullable=False)
    def __repr__(self) -> str: return f"<HistoryMessageORM(id={self.id}, session_id='{self.session_id}')>"

def get_sync_history_session() -> Session: # Tu función para obtener sesión síncrona
    global _sync_history_engine, _SyncHistorySessionLocalFactory
    if _sync_history_engine is None:
        sync_db_url = settings.SYNC_DATABASE_CRUD_URL
        if not sync_db_url: raise ValueError("SYNC_DATABASE_CRUD_URL no configurada para historial.")
        _sync_history_engine = create_engine(sync_db_url, echo=False)
        _HistoryBase.metadata.create_all(_sync_history_engine) 
        _SyncHistorySessionLocalFactory = sessionmaker(autocommit=False, autoflush=False, bind=_sync_history_engine)
        print(f"CHAT_EP_INFO: Engine y SessionFactory SÍNCRONOS para historial (re)creados desde: {sync_db_url[:30]}...")
    if not _SyncHistorySessionLocalFactory: raise RuntimeError("SyncHistorySessionLocalFactory no inicializada.")
    return _SyncHistorySessionLocalFactory()

class FullyCustomChatMessageHistory(BaseChatMessageHistory): # Tu clase actual que funciona
    def __init__(self, session_id: str): # ... (tu código actual)
        if not session_id: raise ValueError("session_id es requerido para el historial.")
        self.session_id = session_id
        print(f"FullyCustomChatMessageHistory: Instanciada para session_id '{self.session_id}'.")

    @property
    def messages(self) -> List[BaseMessage]: # ... (tu código actual)
        db_session: Session = get_sync_history_session()
        try:
            stmt = select(_HistoryMessageORM).where(_HistoryMessageORM.session_id == self.session_id).order_by(_HistoryMessageORM.id.asc())
            orm_messages = db_session.execute(stmt).scalars().all()
            converted_messages: List[BaseMessage] = []
            for orm_msg in orm_messages:
                if isinstance(orm_msg.message, dict): converted_messages.extend(messages_from_dict([orm_msg.message]))
                elif isinstance(orm_msg.message, str):
                    try: message_dict = json.loads(orm_msg.message); converted_messages.extend(messages_from_dict([message_dict]))
                    except json.JSONDecodeError: print(f"HIST_ERROR: Mensaje (str) ID {orm_msg.id} no es JSON válido.")
                else: print(f"HIST_WARNING: Mensaje ID {orm_msg.id} tipo inesperado: {type(orm_msg.message)}")
            if LANGCHAIN_VERBOSE_FLAG: print(f"HIST_DEBUG (FullyCustom): Recuperados {len(converted_messages)} mensajes para session '{self.session_id}'.")
            return converted_messages
        except Exception as e: print(f"HIST_ERROR (FullyCustom - get messages for '{self.session_id}'): {e}"); db_session.rollback(); raise
        finally: db_session.close()

    def add_messages(self, messages: List[BaseMessage]) -> None: # ... (tu código actual)
        db_session: Session = get_sync_history_session()
        try:
            for message in messages:
                message_as_dict = message_to_dict(message)
                orm_entry = _HistoryMessageORM(session_id=self.session_id, message=message_as_dict)
                db_session.add(orm_entry)
            db_session.commit()
            if LANGCHAIN_VERBOSE_FLAG: print(f"HIST_DEBUG (FullyCustom): Añadidos {len(messages)} mensajes para session '{self.session_id}'.")
        except Exception as e: print(f"HIST_ERROR (FullyCustom - add messages for '{self.session_id}'): {e}"); db_session.rollback(); raise
        finally: db_session.close()

    def clear(self) -> None: # ... (tu código actual)
        db_session: Session = get_sync_history_session()
        try:
            stmt = _HistoryMessageORM.__table__.delete().where(_HistoryMessageORM.session_id == self.session_id)
            db_session.execute(stmt); db_session.commit()
            if LANGCHAIN_VERBOSE_FLAG: print(f"HIST_DEBUG (FullyCustom): Historial limpiado para session '{self.session_id}'.")
        except Exception as e: print(f"HIST_ERROR (FullyCustom - clear for '{self.session_id}'): {e}"); db_session.rollback(); raise
        finally: db_session.close()

# --- NUEVO: Wrapper para Historial con Filtrado por Contexto ---
class ContextAwareFilteredHistory(BaseChatMessageHistory):
    def __init__(self, 
                 underlying_history: BaseChatMessageHistory, 
                 allowed_context_names_current: List[str]):
        self.underlying_history = underlying_history
        self.allowed_context_names_current_set = set(allowed_context_names_current) # Para búsqueda O(1)
        self.session_id = getattr(underlying_history, 'session_id', 'unknown_session_for_wrapper') # Copiar session_id si existe
        print(f"ContextAwareFilteredHistory: Instanciada para session '{self.session_id}'. Contextos permitidos: {self.allowed_context_names_current_set}")

    @property
    def messages(self) -> List[BaseMessage]:
        all_messages = self.underlying_history.messages # Llama al .messages del FullyCustom...
        filtered_messages: List[BaseMessage] = []
        
        # Esta lógica intenta mantener pares Human-AI, omitiendo ambos si la AI no es permitida.
        idx = 0
        while idx < len(all_messages):
            current_msg = all_messages[idx]
            is_human_msg = current_msg.type == "human"
            
            if is_human_msg:
                # Si hay un mensaje AI después de este humano
                if (idx + 1) < len(all_messages) and all_messages[idx+1].type == "ai":
                    ai_msg_candidate = all_messages[idx+1]
                    stored_source_names_str = ai_msg_candidate.additional_kwargs.get("source_contexts_names", "")
                    
                    if stored_source_names_str: # El mensaje AI tiene información de contexto de origen
                        stored_source_names_set = set(s.strip() for s in stored_source_names_str.split(',') if s.strip())
                        # Si hay alguna intersección entre los contextos del mensaje y los actualmente permitidos
                        if self.allowed_context_names_current_set.intersection(stored_source_names_set):
                            filtered_messages.append(current_msg) # Añadir humano
                            filtered_messages.append(ai_msg_candidate) # Añadir AI
                        else:
                            # Ningún contexto del mensaje AI está permitido, omitir par Human-AI
                            print(f"CONTEXT_AWARE_HIST_FILTER: Omitiendo par Human-AI. AI_Contexts='{stored_source_names_set}', Allowed='{self.allowed_context_names_current_set}'")
                    else: 
                        # El mensaje AI no tiene metadatos de contexto, incluir el par por defecto
                        filtered_messages.append(current_msg)
                        filtered_messages.append(ai_msg_candidate)
                    idx += 2 # Avanzar de a 2 (human + ai procesados)
                else:
                    # Mensaje humano sin un AI que le siga, incluirlo
                    filtered_messages.append(current_msg)
                    idx += 1
            else:
                # Mensaje que no es humano (ej. AI al inicio, o System), incluirlo si no es AI con contexto no permitido
                # (La lógica actual prioriza el par human-ai, un AI solo se añadiría aquí si no tuvo un Human previo procesado)
                # Para simplificar, podemos añadir AI si no tiene contexto, o si su contexto está permitido.
                # Sin embargo, la estructura Human-AI-Human-AI es más común.
                if current_msg.type == "ai":
                    stored_source_names_str = current_msg.additional_kwargs.get("source_contexts_names", "")
                    if stored_source_names_str:
                        stored_source_names_set = set(s.strip() for s in stored_source_names_str.split(',') if s.strip())
                        if self.allowed_context_names_current_set.intersection(stored_source_names_set):
                            filtered_messages.append(current_msg)
                        else:
                            print(f"CONTEXT_AWARE_HIST_FILTER: Omitiendo AI msg suelto. AI_Contexts='{stored_source_names_set}', Allowed='{self.allowed_context_names_current_set}'")
                    else: # AI sin source_context
                        filtered_messages.append(current_msg)
                else: # System messages, etc.
                    filtered_messages.append(current_msg)
                idx += 1

        if LANGCHAIN_VERBOSE_FLAG or True:
             print(f"ContextAwareFilteredHistory: Originales={len(all_messages)} msgs, Filtrados={len(filtered_messages)} msgs para session '{self.session_id}'.")
        return filtered_messages

    def add_messages(self, messages: List[BaseMessage]) -> None:
        self.underlying_history.add_messages(messages) # Siempre guarda en el historial completo

    def clear(self) -> None:
        self.underlying_history.clear() # Limpia el historial completo

# --- Endpoint Principal del Chat ---
@router.post("/", response_model=ChatResponse)
async def process_chat_message_langchain(
    chat_request: ChatRequest,
    db_crud: AsyncSession = Depends(get_crud_db_session),
    current_api_client: ApiClientModel = Depends(get_validated_api_client)
):
    start_time = time.time(); question = chat_request.message; user_dni_session_id = chat_request.dni
    log_entry_data: Dict[str, Any] = { "user_dni": user_dni_session_id, "api_client_name": current_api_client.name, "user_message": question, "llm_model_used": ACTIVE_LLM_MODEL_NAME, "bot_response": "[Respuesta no generada por error]", "intent": "UNKNOWN", "retrieved_context_summary": None, "full_prompt_to_llm": None, "error_message": None, "metadata_details_json": {"used_sources": []}}
    print(f"CHAT_EP_INFO: Solicitud Chat: Cliente='{current_api_client.name}', UserID='{user_dni_session_id}', Pregunta='{question[:100]}...'")
    
    unfiltered_chat_history: FullyCustomChatMessageHistory
    chat_message_history_for_chain: ContextAwareFilteredHistory # El que usa la cadena RAG

    try:
        # Primero obtener los contextos permitidos actuales para este ApiClient
        api_client_settings = current_api_client.settings or {}
        allowed_context_ids_from_client: List[int] = api_client_settings.get("allowed_context_ids", [])
        resolved_api_client_allowed_context_names: List[str] = []
        if allowed_context_ids_from_client:
            stmt_names = select(ContextDefinition.name).filter(ContextDefinition.id.in_(allowed_context_ids_from_client), ContextDefinition.is_active == True)
            result_names = await db_crud.execute(stmt_names)
            resolved_api_client_allowed_context_names = result_names.scalars().all()
        print(f"CHAT_EP_INFO (pre-historial): Nombres de contextos (activos, permitidos) para ApiClient: {resolved_api_client_allowed_context_names or 'NINGUNO'}")

        # Crear el historial base (sin filtrar)
        unfiltered_chat_history = FullyCustomChatMessageHistory(session_id=user_dni_session_id)
        # Crear el wrapper de historial filtrado que se pasará a la cadena RAG
        chat_message_history_for_chain = ContextAwareFilteredHistory(
            underlying_history=unfiltered_chat_history,
            allowed_context_names_current=resolved_api_client_allowed_context_names
        )
    except Exception as e_hist_init:
        print(f"CHAT_EP_ERROR_FATAL: Fallo inicializando historial (wrapper o base): {e_hist_init}"); traceback.print_exc()
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Error con servicio de historial.")

    if LANGCHAIN_VERBOSE_FLAG: # Carga temprana para debug (ahora usa el filtrado)
        try:
            # .messages del wrapper es el que se usa, así que la llamada verbose mostrará los mensajes filtrados
            loaded_messages_verbose = await asyncio.to_thread(lambda: list(chat_message_history_for_chain.messages))
            print(f"CHAT_EP_DEBUG: Historial FILTRADO (verbose): {len(loaded_messages_verbose)} mensajes para '{user_dni_session_id}'.")
        except Exception as e_load_hist_verbose:
            print(f"CHAT_EP_WARNING: Error cargando historial FILTRADO (verbose): {type(e_load_hist_verbose).__name__} - {e_load_hist_verbose}")

    try:
        llm = get_langchain_llm()
        # La obtención de resolved_api_client_allowed_context_names se movió arriba, antes de crear el historial filtrado.

        question_lower = question.lower()
        is_sql_intent_heuristic = any(k in question_lower for k in SQL_INTENT_KEYWORDS) or any(p.lower() in question_lower for p in DW_TABLE_PREFIXES_FOR_INTENT)
        is_dw_context_allowed = DW_CONTEXT_CONFIG_NAME in resolved_api_client_allowed_context_names
        attempt_sql_query_flag = is_dw_context_allowed and is_sql_intent_heuristic
        
        print(f"CHAT_EP_DEBUG_ROUTE: Pregunta (lower): '{question_lower}'")
        print(f"CHAT_EP_DEBUG_ROUTE: Intención SQL Heurística = {is_sql_intent_heuristic}")
        print(f"CHAT_EP_DEBUG_ROUTE: Contexto DW ('{DW_CONTEXT_CONFIG_NAME}') Permitido? {is_dw_context_allowed}")
        print(f"CHAT_EP_DEBUG_ROUTE: ¿Intentar query SQL (attempt_sql_query_flag)? {attempt_sql_query_flag}")
        
        context_def_dw_instance_orm: Optional[ContextDefinition] = None
        if is_dw_context_allowed:
             context_def_dw_instance_orm = await crud_context_definition.get_context_definition_by_name(db_crud, name=DW_CONTEXT_CONFIG_NAME, load_relations_fully=True)
             if context_def_dw_instance_orm: print(f"CHAT_EP_DEBUG_ROUTE: DW Context ('{context_def_dw_instance_orm.name}') Cargado. MainType: {context_def_dw_instance_orm.main_type}, DBConnID: {context_def_dw_instance_orm.db_connection_config_id}")
             else: print(f"CHAT_EP_WARNING_ROUTE: Contexto DW '{DW_CONTEXT_CONFIG_NAME}' permitido pero NO encontrado/inactivo.")
        
        # ---- Rama Text-to-SQL ----
        if attempt_sql_query_flag and context_def_dw_instance_orm and context_def_dw_instance_orm.is_active and \
           context_def_dw_instance_orm.main_type == ContextMainType.DATABASE_QUERY and context_def_dw_instance_orm.db_connection_config_id:
            # (Tu lógica Text-to-SQL como la tenías, asegurando que `chat_history_str` se genere del historial filtrado si es necesario)
            print(f"CHAT_EP_INFO: ===== ENTRANDO A RAMA TEXT-TO-SQL ====="); log_entry_data["intent"] = "SQL_QUERY_DW" #... (el resto igual)

            db_conn_orm: Optional[DatabaseConnectionConfig] = get_db_connection_by_id_sync(context_def_dw_instance_orm.db_connection_config_id)
            if not db_conn_orm: raise ValueError(f"Config BD ID '{context_def_dw_instance_orm.db_connection_config_id}' para DW no encontrada.")
            sql_policy_from_cfg = (context_def_dw_instance_orm.processing_config or {}).get("sql_select_policy", {})
            if not sql_policy_from_cfg or not isinstance(sql_policy_from_cfg.get("allowed_tables_for_select"), list) or not sql_policy_from_cfg.get("allowed_tables_for_select"):
                 log_entry_data["bot_response"] = "La configuración para consultas SQL (política de tablas permitidas) está incompleta o vacía."
                 log_entry_data["error_message"] = "Política SQL: Falta 'allowed_tables_for_select' o está vacía en processing_config.sql_select_policy."
                 print(f"CHAT_EP_ERROR_SQL_POLICY: {log_entry_data['error_message']}\n  Contenido de sql_policy_from_cfg: {sql_policy_from_cfg}")
            else:
                sql_chat_history_for_llm: str = ""
                if CHAT_HISTORY_WINDOW_SIZE_SQL > 0:
                    # Usar el historial FILTRADO si se decide pasarlo al Text-to-SQL
                    sql_hist_messages = await asyncio.to_thread(lambda: list(chat_message_history_for_chain.messages)) 
                    if sql_hist_messages:
                        limited_hist = sql_hist_messages[-(CHAT_HISTORY_WINDOW_SIZE_SQL * 2):]
                        sql_chat_history_for_llm = "\n".join([f"{msg.type.upper()}: {msg.content}" for msg in limited_hist])
                sql_output = await run_text_to_sql_lcel_chain(question=question, chat_history_str=sql_chat_history_for_llm, db_conn_config_for_sql=db_conn_orm, llm=llm, sql_policy=sql_policy_from_cfg)
                log_entry_data["bot_response"] = sql_output["final_answer_llm"]
                log_entry_data["metadata_details_json"]["used_sources"].append({"type": "DATABASE_QUERY_RESULT", "context_name": context_def_dw_instance_orm.name, "source_identifier": "Direct_SQL_Execution", "details": {"generated_sql": sql_output.get('generated_sql'), "dw_connection_used": db_conn_orm.name }})
                log_entry_data["retrieved_context_summary"] = f"Respuesta desde BD ({db_conn_orm.name}). SQL: {str(sql_output.get('generated_sql'))[:100]}..."

        # ---- Rama RAG Documental / Esquema ----
        else: # (Tu lógica RAG como la tenías, pero usando chat_message_history_for_chain para la memoria)
            print(f"CHAT_EP_INFO: ===== ENTRANDO A RAMA RAG (SQL NO CUMPLIÓ CONDICIONES Ó INTENTO SQL FUE FALSE) ====="); #... (tus prints de razón RAG)
            # ... (Tu lógica para `final_rag_target_context_names` y `primary_context_for_rag_prompts_orm` igual)
            final_rag_target_context_names: List[str] = [] # Ejemplo
            primary_context_for_rag_prompts_orm: Optional[ContextDefinition] = None # Ejemplo
            for ctx_name in resolved_api_client_allowed_context_names: #... (tu bucle para popular estas dos variables)
                # ...
                pass

            if not final_rag_target_context_names: log_entry_data["bot_response"] = "No se encontraron contextos documentales válidos." # ...
            else:
                # IMPORTANTE: Usar el historial FILTRADO para la memoria RAG
                rag_memory = ConversationBufferWindowMemory(memory_key="chat_history", chat_memory=chat_message_history_for_chain, return_messages=True, k=CHAT_HISTORY_WINDOW_SIZE_RAG, output_key='answer')
                # ... (el resto de tu configuración y llamada a ConversationalRetrievalChain igual)
                # Asegúrate de actualizar el loggeo de `metadata_details` con los documentos recuperados
                conversational_rag_chain = ConversationalRetrievalChain(...) # Tu cadena completa
                rag_result = await asyncio.to_thread(conversational_rag_chain.invoke, {"question": question})
                log_entry_data["bot_response"] = rag_result.get("answer", "[Respuesta RAG no obtenida]")
                # ... tu lógica de loggeo de retrieved_docs para metadata_details ...
    
    # --- Manejo de Excepciones (igual que antes) ---
    # ...
    except Exception as e_uncaught: print(f"CHAT_EP_CRITICAL_ERROR: Excepción general: {type(e_uncaught).__name__} - {e_uncaught}"); traceback.print_exc(); log_entry_data["error_message"] = f"Error Interno: {type(e_uncaught).__name__}"; log_entry_data["bot_response"] = "[Chatbot tuvo problema técnico.]"
    # --- Bloque Finally para Guardado ---
    finally:
        ai_message_metadata_for_history_storage: Dict[str, Any] = {}
        # Intentar determinar el (los) contexto(s) que generaron la respuesta para almacenarlo.
        # Esta lógica es crucial para que el ContextAwareFilteredHistory funcione.
        intent_final = log_entry_data.get("intent")
        if intent_final == "SQL_QUERY_DW":
            # En este punto, context_def_dw_instance_orm debería estar definido si la rama SQL tuvo éxito.
            # Es más seguro tomar el nombre directamente de settings si no tenemos el objeto ORM.
            ai_message_metadata_for_history_storage["source_contexts_names"] = DW_CONTEXT_CONFIG_NAME
        elif intent_final == "RAG_DOCS_OR_SCHEMA":
            # Usar los contextos que realmente se usaron para el RAG (final_rag_target_context_names)
            # Es importante que final_rag_target_context_names esté disponible aquí.
            # Si la variable se definió solo dentro del 'else' de la rama RAG, no estará disponible.
            # Podrías guardar los nombres usados para RAG en log_entry_data también.
            # O si "metadata_details_json.used_sources" ya tiene context_name:
            rag_sources = log_entry_data.get("metadata_details_json", {}).get("used_sources", [])
            used_rag_context_names = sorted(list(set(src.get("context_name") for src in rag_sources if src.get("context_name"))))
            if used_rag_context_names:
                ai_message_metadata_for_history_storage["source_contexts_names"] = ", ".join(used_rag_context_names)
            # Si no, es una respuesta genérica de RAG sin fuentes específicas, no añadimos source_contexts_names

        history_writer_for_saving: Optional[FullyCustomChatMessageHistory] = None
        if log_entry_data["error_message"] is None and "[Respuesta no generada por error]" not in log_entry_data["bot_response"]:
            try:
                print(f"CHAT_EP_DEBUG: Guardando interacción en historial (FullyCustom) con metadata: {ai_message_metadata_for_history_storage}")
                messages_to_add_to_hist = [
                    HumanMessage(content=question), 
                    AIMessage(content=log_entry_data["bot_response"], additional_kwargs=ai_message_metadata_for_history_storage)
                ]
                history_writer_for_saving = FullyCustomChatMessageHistory(session_id=user_dni_session_id)
                await asyncio.to_thread(history_writer_for_saving.add_messages, messages_to_add_to_hist)
                print(f"CHAT_EP_INFO: Interacción guardada en historial (FullyCustom).")
            except Exception as e_hist_save_final: print(f"CHAT_EP_ERROR: Falló guardado final historial: {e_hist_save_final}")
        
        end_time = time.time(); log_entry_data["response_time_ms"] = int((end_time - start_time) * 1000)
        try: # (tu log de interacción sin cambios)
            print(f"CHAT_EP_DEBUG: Guardando log de interacción. Intent: '{log_entry_data.get('intent', 'UNKNOWN')}'")
            await crud_interaction_log.create_interaction_log(db_crud, log_entry_data)
            print("CHAT_EP_INFO: Log de interacción guardado.")
        except Exception as e_log_save_final: print(f"CHAT_EP_CRITICAL_ERROR: Falló guardado log: {e_log_save_final}")
            
    return ChatResponse(dni=chat_request.dni,original_message=question,bot_response=log_entry_data["bot_response"].strip(), metadata_details_json=log_entry_data.get("metadata_details_json"))

# --- Endpoint Auxiliar (Listar Modelos) ---
@router.get("/list-models", summary="Listar Modelos LLM Disponibles (Gemini)", deprecated=True)
async def list_available_llm_models(): # Tu código igual...
    if not GEMINI_API_KEY_ENV: raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="API Key no configurada.")
    models_info = []; 
    try:
        for m in genai.list_models(): 
            if 'generateContent' in m.supported_generation_methods:
                models_info.append({"name": m.name, "version": m.version, "display_name": m.display_name,"description": m.description, "input_token_limit": m.input_token_limit,"output_token_limit": m.output_token_limit})
        return models_info
    except Exception as e: raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error listando modelos: {str(e)}")

print("CHAT_EP_DEBUG: chat_api_endpoints.py module FULLY loaded.")